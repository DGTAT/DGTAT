{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093ebe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44930c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):#长L，索引idx mask1\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=bool)\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    #coo格式变为元组表示\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):#不是coo变成coo格式\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()#行堆叠矩阵，再转置，变成节点对（边）\n",
    "        values = mx.data #节点属性\n",
    "        shape = mx.shape #结构\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list): \n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))      \n",
    "    r_inv = np.power(rowsum, -1).flatten()      #归一标准化\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)                 #对角化\n",
    "    features = r_mat_inv.dot(features)          #点乘，标准化\n",
    "    return features.todense(), sparse_to_tuple(features)#变为元组表示\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()#1/sqrt(d)\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)#D-1/2 A D-1/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36492aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))#按行读取 消除空白符\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea722f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d4b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_str):\n",
    "      # \"\"\"数据处理训练集(x,y); 测试集(tx,ty); 评估集(allx,ally) ; 邻接图graph\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/{}/ind.{}.{}\".format(dataset_str,dataset_str, names[i]), 'rb') as f:#打开文件ind.数据集.?\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))              #load反序列化对象，将文件中的数据解析为一个python对象\n",
    "            #依次将对象读入objects\n",
    "\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    #citeseer: x120(训练) tx1000(测试) allx2312\n",
    "    #cora: x140(训练) tx1000(测试) allx1708\n",
    "    #print(graph)\n",
    "    #print(x.shape[0],tx.shape[0],allx.shape[0])\n",
    "\n",
    "    test_idx_reorder = parse_index_file(\"data/{}/ind.{}.test.index\".format(dataset_str,dataset_str))#处理index文件并返回index矩阵\n",
    "    test_idx_range = np.sort(test_idx_reorder)#重排索引\n",
    "    \n",
    "    #print(test_idx_range)#1708~2707(最后1000个为测试集)\n",
    "    \n",
    "    if dataset_str == 'citeseer':\n",
    "        \n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()#全部数据\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]#索引重排\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph)) \n",
    "    #print(adj,np.shape(adj))#3327*3327、\n",
    "    #test_adj=np.array(adj.todense())   \n",
    "    #print((test_adj==test_adj.transpose()).all())#无向图\n",
    "    labels = np.vstack((ally, ty))#所有标签\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "    idx_train = range(len(y))\n",
    "    idx_test = test_idx_range\n",
    "    \n",
    "    '''\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "    '''\n",
    "    \n",
    "    adj = adj.astype(np.float32)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))#A+I 标准化\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    \n",
    "    if dataset_str == 'citeseer':\n",
    "        for i in range(2312,3327):\n",
    "            if i not in test_idx_range:\n",
    "                labels[i][0]=1\n",
    "        labels = torch.LongTensor(np.where(labels>0)[1])\n",
    "        for i in range(2312,3327):\n",
    "            if i not in test_idx_range:\n",
    "                labels[i]=0\n",
    "    else:\n",
    "        labels = torch.LongTensor(np.where(labels>0)[1])\n",
    "    \n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)    \n",
    "    \n",
    "    return adj, features,idx_train, idx_val, idx_test, labels\n",
    "    #return adj, features,idx_train, idx_val, idx_test, train_mask, val_mask, test_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f203da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82705\\AppData\\Local\\Temp\\ipykernel_8716\\624335382.py:8: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
      "        134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
      "        148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n",
      "        162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
      "        176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
      "        190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
      "        204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
      "        218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
      "        232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245,\n",
      "        246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
      "        274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n",
      "        288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301,\n",
      "        302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "        316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "        330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
      "        344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357,\n",
      "        358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371,\n",
      "        372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
      "        386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399,\n",
      "        400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413,\n",
      "        414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427,\n",
      "        428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
      "        442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
      "        456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469,\n",
      "        470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n",
      "        484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
      "        498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511,\n",
      "        512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "        526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539,\n",
      "        540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
      "        554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567,\n",
      "        568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581,\n",
      "        582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595,\n",
      "        596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609,\n",
      "        610, 611, 612, 613, 614, 615, 616, 617, 618, 619])\n",
      "tensor([2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323,\n",
      "        2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335,\n",
      "        2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347,\n",
      "        2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359,\n",
      "        2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371,\n",
      "        2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383,\n",
      "        2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395,\n",
      "        2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2408,\n",
      "        2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420,\n",
      "        2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432,\n",
      "        2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444,\n",
      "        2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456,\n",
      "        2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468,\n",
      "        2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480,\n",
      "        2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2490, 2491, 2492, 2493,\n",
      "        2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505,\n",
      "        2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517,\n",
      "        2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529,\n",
      "        2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541,\n",
      "        2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2554,\n",
      "        2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566,\n",
      "        2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578,\n",
      "        2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590,\n",
      "        2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602,\n",
      "        2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614,\n",
      "        2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626,\n",
      "        2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638,\n",
      "        2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650,\n",
      "        2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662,\n",
      "        2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674,\n",
      "        2675, 2676, 2677, 2678, 2679, 2680, 2681, 2683, 2684, 2685, 2686, 2687,\n",
      "        2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699,\n",
      "        2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711,\n",
      "        2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723,\n",
      "        2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735,\n",
      "        2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747,\n",
      "        2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759,\n",
      "        2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771,\n",
      "        2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2782, 2783, 2784,\n",
      "        2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796,\n",
      "        2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808,\n",
      "        2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820,\n",
      "        2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832,\n",
      "        2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844,\n",
      "        2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856,\n",
      "        2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868,\n",
      "        2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880,\n",
      "        2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892,\n",
      "        2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904,\n",
      "        2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916,\n",
      "        2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928,\n",
      "        2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940,\n",
      "        2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952,\n",
      "        2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965,\n",
      "        2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977,\n",
      "        2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989,\n",
      "        2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001,\n",
      "        3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013,\n",
      "        3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025,\n",
      "        3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037,\n",
      "        3038, 3039, 3040, 3041, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050,\n",
      "        3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062,\n",
      "        3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075,\n",
      "        3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087,\n",
      "        3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099,\n",
      "        3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111,\n",
      "        3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123,\n",
      "        3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135,\n",
      "        3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147,\n",
      "        3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159,\n",
      "        3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171,\n",
      "        3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183,\n",
      "        3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195,\n",
      "        3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207,\n",
      "        3208, 3209, 3210, 3211, 3213, 3215, 3216, 3217, 3218, 3219, 3220, 3221,\n",
      "        3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233,\n",
      "        3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245,\n",
      "        3246, 3247, 3248, 3249, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258,\n",
      "        3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270,\n",
      "        3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282,\n",
      "        3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3293, 3294, 3295,\n",
      "        3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3307, 3308, 3310,\n",
      "        3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322,\n",
      "        3323, 3324, 3325, 3326])\n"
     ]
    }
   ],
   "source": [
    "#citeseer_adj, features,  idx_train, idx_val, idx_test, labels= load_data(\"citeseer\")\n",
    "#citeseer_adj, features,  idx_train, idx_val, idx_test, train_mask, val_mask, test_mask, labels= load_data(\"citeseer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e49ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
